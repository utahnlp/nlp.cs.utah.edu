@inproceedings{jobanputra-etal-2024-universal,
    title = "A {U}niversal {D}ependencies Treebank for {G}ujarati",
    author = {Jobanputra, Mayank  and
      Mehta, Maitrey  and
      {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i}},
    editor = {Bhatia, Archna  and
      Bouma, Gosse  and
      Do{\u{g}}ru{\"o}z, A. Seza  and
      Evang, Kilian  and
      Garcia, Marcos  and
      Giouli, Voula  and
      Han, Lifeng  and
      Nivre, Joakim  and
      Rademaker, Alexandre},
    booktitle = "Proceedings of the Joint Workshop on Multiword Expressions and Universal Dependencies (MWE-UD) @ LREC-COLING 2024",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.mwe-1.9",
    pages = "56--62",
    abstract = "The Universal Dependencies (UD) project has presented itself as a valuable platform to develop various resources for the languages of the world. We present and release a sample treebank for the Indo-Aryan language of Gujarati {--} a widely spoken language with little linguistic resources. This treebank is the first labeled dataset for dependency parsing in the language and the script (the Gujarati script). The treebank contains 187 part-of-speech and dependency annotated sentences from diverse genres. We discuss various idiosyncratic examples, annotation choices and present an elaborate corpus along with agreement statistics. We see this work as a valuable resource and a stepping stone for research in Gujarati Computational Linguistics.",
}

@inproceedings{mehta-etal-2024-promptly,
    title = "Promptly Predicting Structures: The Return of Inference",
    author = "Mehta, Maitrey  and
      Pyatkin, Valentina  and
      Srikumar, Vivek",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.7",
    doi = "10.18653/v1/2024.naacl-long.7",
    pages = "112--130",
    abstract = "Prompt-based methods have been used extensively across NLP to build zero- and few-shot label predictors. Many NLP tasks are naturally structured: that is, their outputs consist of multiple labels which constrain each other. Annotating data for such tasks can be cumbersome. Can the promise of the prompt-based paradigm be extended to such structured outputs? In this paper, we present a framework for constructing zero- and few-shot linguistic structure predictors. Our key insight is that we can use structural constraints{---}and combinatorial inference derived from them{---}to filter out inconsistent structures predicted by large language models. We instantiated this framework on two structured prediction tasks, and five datasets. Across all cases, our results show that enforcing consistency not only constructs structurally valid outputs, but also improves performance over the unconstrained variants.",
}

@inproceedings{xu-etal-2024-context,
    title = "In-Context Example Ordering Guided by Label Distributions",
    author = "Xu, Zhichao  and
      Cohen, Daniel  and
      Wang, Bei  and
      Srikumar, Vivek",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.167",
    doi = "10.18653/v1/2024.findings-naacl.167",
    pages = "2623--2640",
    abstract = "By allowing models to predict without task-specific training, in-context learning (ICL) with pretrained LLMs has enormous potential in NLP. However, a number of problems persist in ICL. In particular, its performance is sensitive to the choice and order of in-context examples. Given the same set of in-context examples with different orderings, model performance may vary from near random to near state-of-the-art. In this work, we formulate in-context example ordering as an optimization problem. We examine three problem settings that differ in the assumptions they make about what is known about the task. Inspired by the idea of learning from label proportions, we propose two principles for in-context example ordering guided by model{'}s probability predictions. We apply our proposed principles to thirteen text classification datasets and nine different autoregressive LLMs with 700M to 13B parameters. We demonstrate that our approach outperforms the baselines by improving the classification accuracy, reducing model miscalibration, and also by selecting better in-context examples.",
}

@article{
    bentham2024chainofthought,
    title={Chain-of-Thought Unfaithfulness as Disguised Accuracy},
    author={Oliver Bentham and Nathan Stringham and Ana Marasovi{\'c}},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    month = jul,
    year={2024},
    url={https://openreview.net/forum?id=ydcrP55u2e},
    note={Reproducibility Certification}
}

@inproceedings{hashemi-chaleshtori-etal-2024-evaluating,
    title = "On Evaluating Explanation Utility for Human-{AI} Decision Making in {NLP}",
    author = "Hashemi Chaleshtori, Fateme  and
      Ghosal, Atreya  and
      Gill, Alexander  and
      Bambroo, Purbid  and
      Marasovi{\'c}, Ana",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.439/",
    doi = "10.18653/v1/2024.findings-emnlp.439",
    pages = "7456--7504",
    abstract = "Is explainability a false promise? This debate has emerged from the insufficient evidence that explanations help people in situations they are introduced for. More human-centered, application-grounded evaluations of explanations are needed to settle this. Yet, with no established guidelines for such studies in NLP, researchers accustomed to standardized proxy evaluations must discover appropriate measurements, tasks, datasets, and sensible models for human-AI teams in their studies. To aid with this, we first review existing metrics suitable for application-grounded evaluation. We then establish criteria to select appropriate datasets, and using them, we find that only 4 out of over 50 datasets available for explainability research in NLP meet them. We then demonstrate the importance of reassessing the state of the art to form and study human-AI teams: teaming people with models for certain tasks might only now start to make sense, and for others, it remains unsound. Finally, we present the exemplar studies of human-AI decision-making for one of the identified tasks {---} verifying the correctness of a legal claim given a contract. Our results show that providing AI predictions, with or without explanations, does not cause decision makers to speed up their work without compromising performance. We argue for revisiting the setup of human-AI teams and improving automatic deferral of instances to AI, where explanations could play a useful role."
}


@inproceedings{xu-etal-2024-beyond-perplexity,
    title = "Beyond Perplexity: Multi-dimensional Safety Evaluation of {LLM} Compression",
    author = "Xu, Zhichao  and
      Gupta, Ashim  and
      Li, Tao  and
      Bentham, Oliver  and
      Srikumar, Vivek",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.901/",
    doi = "10.18653/v1/2024.findings-emnlp.901",
    pages = "15359--15396",
    abstract = "Increasingly, model compression techniques enable large language models (LLMs) to be deployed in real-world applications. As a result of this momentum towards local deployment, compressed LLMs will interact with a large population. Prior work on compression typically prioritize preserving perplexity, which is directly analogous to training loss. The impact of compression method on other critical aspects of model behavior{---}particularly safety{---}requires systematic assessment. To this end, we investigate the impact of model compression along four dimensions: (1) degeneration harm, i.e., bias and toxicity in generation; (2) representational harm, i.e., biases in discriminative tasks; (3) dialect bias; and (4) language modeling and downstream task performance. We examine a wide spectrum of LLM compression techniques, including unstructured pruning, semi-structured pruning, and quantization. Our analysis reveals that compression can lead to unexpected consequences. Although compression may unintentionally alleviate LLMs' degeneration harm, it can still exacerbate representational harm. Furthermore, increasing compression produces a divergent impact on different protected groups. Finally, different compression methods have drastically different safety impacts: for example, quantization mostly preserves bias while pruning degrades quickly. Our findings underscore the importance of integrating safety assessments into the development of compressed LLMs to ensure their reliability across real-world applications."
}


@inproceedings{gupta-etal-2024-whispers,
    title = "Whispers of Doubt Amidst Echoes of Triumph in {NLP} Robustness",
    author = "Gupta, Ashim  and
      Rajendhran, Rishanth  and
      Stringham, Nathan  and
      Srikumar, Vivek  and
      Marasovi{\'c}, Ana",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.310",
    doi = "10.18653/v1/2024.naacl-long.310",
    pages = "5533--5590",
    abstract = "*Do larger and more performant models resolve NLP{'}s longstanding robustness issues?* We investigate this question using over 20 models of different sizes spanning different architectural choices and pretraining objectives. We conduct evaluations using (a) out-of-domain and challenge test sets, (b) behavioral testing with CheckLists, (c) contrast sets, and (d) adversarial inputs. Our analysis reveals that not all out-of-domain tests provide insight into robustness. Evaluating with CheckLists and contrast sets shows significant gaps in model performance; merely scaling models does not make them adequately robust. Finally, we point out that current approaches for adversarial evaluations of models are themselves problematic: they can be easily thwarted, and in their current forms, do not represent a sufficiently deep probe of model robustness. We conclude that not only is the question of robustness in NLP as yet unresolved, but even some of the approaches to measure robustness need to be reassessed.",
}


@article{mohanty2024crystext,
  title={CrysText: A Generative AI Approach for Text-Conditioned Crystal Structure Generation using LLM},
  author={Mohanty, Trupti and Mehta, Maitrey and Sayeed, Hasan M and Srikumar, Vivek and Sparks, Taylor D},
  year={2024},
  month=dec,
  url="https://chemrxiv.org/engage/chemrxiv/article-details/6753874c7be152b1d02eecb5"
}

@inproceedings{xu-jiang-2024-multi,
    title = "Multi-dimensional Evaluation of Empathetic Dialogue Responses",
    author = "Xu, Zhichao  and
      Jiang, Jiepu",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.113/",
    doi = "10.18653/v1/2024.findings-emnlp.113",
    pages = "2066--2087",
    abstract = "Empathy is critical for effective and satisfactory conversational communication. Prior efforts to measure conversational empathy mostly focus on expressed communicative intents{---}that is, the way empathy is expressed. Yet, these works ignore the fact that conversation is also a collaboration involving both speakers and listeners. In contrast, we propose a multi-dimensional empathy evaluation framework to measure both expressed intents from the speaker`s perspective and perceived empathy from the listener`s perspective. We apply our analytical framework to examine internal customer-service dialogues. We find the two dimensions (expressed intent types and perceived empathy) are interconnected, while perceived empathy has high correlations with dialogue satisfaction levels.To reduce the annotation cost, we explore different options to automatically measure conversational empathy: prompting LLMs and training language model-based classifiers. Our experiments show that prompting methods with even popular models like GPT-4 and Flan family models perform relatively poorly on both public and our internal datasets. In contrast, instruction-finetuned classifiers based on FlanT5 family models outperform prior works and competitive baselines. We conduct a detailed ablation study to give more insights into instruction finetuning method`s strong performance."
}


@article{xu2024rankmamba,
  title={RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers},
  author={Xu, Zhichao},
  journal={arXiv preprint arXiv:2403.18276},
  year={2024},
  month=mar,
  url="https://arxiv.org/abs/2403.18276"
}

@inproceedings{wang2024depth,
  title={An in-depth investigation of user response simulation for conversational search},
  author={Wang, Zhenduo and Xu, Zhichao and Srikumar, Vivek and Ai, Qingyao},
  booktitle={Proceedings of the ACM on Web Conference 2024},
  pages={1407--1418},
  year={2024},
  month=apr,
  url="https://dl.acm.org/doi/10.1145/3589334.3645447"
}


@inproceedings{10.1145/3664190.3672508,
author = {Xu, Zhichao and Lamba, Hemank and Ai, Qingyao and Tetreault, Joel and Jaimes, Alex},
title = {CFE2: Counterfactual Editing for Search Result Explanation},
year = {2024},
isbn = {9798400706813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664190.3672508},
doi = {10.1145/3664190.3672508},
abstract = {Search Result Explanation (SeRE) aims to improve search sessions' effectiveness and efficiency by helping users interpret documents' relevance. Existing works mostly focus on factual explanation, i.e. to find/generate supporting evidence about documents' relevance to search queries. However, research in cognitive sciences has shown that human explanations are contrastive i.e. people explain an observed event using some counterfactual events; such explanations reduce cognitive load and provide actionable insights. Though already proven effective in machine learning and NLP communities, there lacks a strict formulation on how counterfactual explanations should be defined and structured, in the context of web search. In this paper, we first discuss the possible formulation of counterfactual explanations in the IR context. Next, we formulate a suite of desiderata for counterfactual explanation in SeRE task and corresponding automatic metrics. With this desiderata, we propose a method named CounterFactual Editing for Search Research Explanation (CFE2). CFE2 provides pairwise counterfactual explanations for document pairs within a search engine result page. Our experiments on five public search datasets demonstrate that CFE2can significantly outperform baselines in both automatic metrics and human evaluations.},
booktitle = {Proceedings of the 2024 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {145–155},
numpages = {11},
keywords = {counterfactual explanation, information retrieval},
location = {Washington DC, USA},
series = {ICTIR '24},
month=aug
}


@inproceedings{10.1145/3627673.3679804,
author = {Zhong, Da and Wang, Xiuling and Xu, Zhichao and Xu, Jun and Wang, Wendy Hui},
title = {Interaction-level Membership Inference Attack against Recommender Systems with Long-tailed Distribution},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679804},
doi = {10.1145/3627673.3679804},
abstract = {Recommender systems (RSs) are susceptible to Interaction-level Membership Inference Attacks (IMIAs), which aim to determine whether specific user-item interactions are present in the training data of the target RS. However, existing IMIAs struggle with inferring the membership of tail interactions, i.e., the interactions involving tail items, due to the limited information available about these items. This paper introduces MINER, a new IMIA designed to enhance attack performance against RSs with long-tailed item distribution. MINER addresses the information scarcity of tail items at both the feature and sample levels. At the feature level, MINER leverages the Knowledge Graphs (KGs) to obtain the auxiliary knowledge of tail items. At the sample level, MINER designs a Bilateral-Branch Network (BBN) as the attack model. The BBN trains two branches independently, with one branch trained on interaction samples with the original long-tailed item distribution and the other on interaction samples with a more balanced item distribution. The outputs of the two branches are aggregated using a cumulative learning component. Our experimental results demonstrate that MINER significantly enhances the attack accuracy of IMIA, especially for tail interactions. Beyond attack design, we design a defense mechanism named RGL to defend against MINER. Empirical evaluations demonstrate that RGL effectively mitigates the privacy risks posed by MINER while preserving recommendation accuracy. Our code is available at https://github.com/dzhong2/MINER.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {3433–3442},
numpages = {10},
keywords = {long-tailed distribution, membership inference attack, privacy of machine learning, recommender system},
location = {Boise, ID, USA},
series = {CIKM '24},
month=oct
}


@article{xu2024state,
  title={State Space Models are Strong Text Rerankers},
  author={Xu, Zhichao and Yan, Jinghua and Gupta, Ashim and Srikumar, Vivek},
  journal={arXiv preprint arXiv:2412.14354},
  year={2024},
  month=dec,
  url="https://arxiv.org/abs/2412.14354"
}

@article{chang2024main,
  title={MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation},
  author={Chang, Chia-Yuan and Jiang, Zhimeng and Rakesh, Vineeth and Pan, Menghai and Yeh, Chin-Chia Michael and Wang, Guanchu and Hu, Mingzhi and Xu, Zhichao and Zheng, Yan and Das, Mahashweta and others},
  journal={arXiv preprint arXiv:2501.00332},
  year={2024},
  month=dec,
  url="https://arxiv.org/abs/2501.00332"
}

@inproceedings{gupta-etal-2024-enhancing,
    title = "Enhancing Question Answering on Charts Through Effective Pre-training Tasks",
    author = "Gupta, Ashim  and
      Gupta, Vivek  and
      Zhang, Shuo  and
      He, Yujie  and
      Zhang, Ning  and
      Shah, Shalin",
    editor = "Belinkov, Yonatan  and
      Kim, Najoung  and
      Jumelet, Jaap  and
      Mohebbi, Hosein  and
      Mueller, Aaron  and
      Chen, Hanjie",
    booktitle = "Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.blackboxnlp-1.11/",
    doi = "10.18653/v1/2024.blackboxnlp-1.11",
    pages = "185--192",
    abstract = "To completely understand a document, the use of textual information is not enough. Understanding visual cues, such as layouts and charts, is also required. While the current state-of-the-art approaches for document understanding (both OCR-based and OCR-free) work well, a thorough analysis of their capabilities and limitations has not yet been performed. Therefore, in this work, we addresses the limitation of current VisualQA models when applied to charts and plots. To investigate shortcomings of the state-of-the-art models, we conduct a comprehensive behavioral analysis, using ChartQA as a case study. Our findings indicate that existing models particularly underperform in answering questions related to the chart`s structural and visual context, as well as numerical information. To address these issues, we propose three simple pre-training tasks that enforce the existing model in terms of both structural-visual knowledge, as well as its understanding of numerical questions. We evaluate our pre-trained model (called MatCha-v2) on three chart datasets - both extractive and abstractive question datasets - and observe that it achieves an average improvement of 1.7 {\%} over the baseline model."
}


@article{gupta2024empirical,
  title={An Empirical Investigation of Matrix Factorization Methods for Pre-trained Transformers},
  author={Gupta, Ashim and Saravani, Sina Mahdipour and Sadayappan, P and Srikumar, Vivek},
  journal={arXiv preprint arXiv:2406.11307},
  year={2024},
  month=jun,
  url="https://arxiv.org/abs/2406.11307"
}


@inproceedings{10.1145/3641525.3663625,
author = {Zheng, Guineng and Ricci, Robert and Srikumar, Vivek},
title = {LogFlux: A Software Suite for Replicating Results in Automated Log Parsing},
year = {2024},
month = jul,
isbn = {9798400705304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641525.3663625},
doi = {10.1145/3641525.3663625},
abstract = {Logging mechanisms are a cornerstone in the development and maintenance of computer systems, gaining even greater prominence in the era of large-scale cloud-based applications. Their critical role in real-time system monitoring and behavior analysis cannot be overstated, making them invaluable tools for system administrators and developers alike. Automated parsing of log messages—turning the text of log messages into parsed, structured data—is a significant research area. Despite the number of log parsers that have emerged over the years, there has been a noticeable gap in the evaluation of these tools, reproduction of results, and direct comparisons between them on a level playing field. Recognizing this, we re-implemented twelve of the most popular log parsers from scratch, enabling them to be used in replication studies. This paper presents our open-source project LogFlux, which is a suite for evaluating automated log parsers so that studies involving them can be replicated. Through LogFlux, we aim to bridge the gap between theoretical log parsing methods and their practical application, offering a robust and easy to use solution that is accessible and effective for a range of users. Our experience in attempting to obtain results from many published log parser algorithms has shed light on important aspects of replication, such as the value of independent implementation for uncovering bugs and the need for careful software engineering to facilitate maintenance.},
booktitle = {Proceedings of the 2nd ACM Conference on Reproducibility and Replicability},
pages = {64–74},
numpages = {11},
keywords = {Implementations, Log Parsing},
location = {Rennes, France},
series = {ACM REP '24}
}


@article{richardson2024understanding,
  title={Understanding the Logic of Direct Preference Alignment through Logic},
  author={Richardson, Kyle and Srikumar, Vivek and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2412.17696},
  year={2024},
  month=dec,
  url="https://arxiv.org/abs/2412.17696"
}

@article{kuo2024identification,
  title={Identification of cultural conversations in therapy using natural language processing models.},
  author={Kuo, Patty B and Mehta, Maitrey and Hashtpari, Halleh and Srikumar, Vivek and Tanana, Michael J and Tao, Karen W and Drinane, Joanna M and Van-Epps, Jake and Imel, Zac E},
  journal={Psychotherapy},
  year={2024},
  publisher={Educational Publishing Foundation},
  month=oct,
  url="https://psycnet.apa.org/record/2025-35447-001"
}

